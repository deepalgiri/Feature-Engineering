{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering**"
      ],
      "metadata": {
        "id": "0fR8fl-Vf_Bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is a parameter?**\n",
        "\n",
        "* A parameter is a numerical value that describes a characteristic of a population in statistics.\n",
        "\n",
        "**Key Points:**\n",
        "* A population includes all possible observations (e.g., all employees in a company).\n",
        "\n",
        "* A parameter is a fixed value that summarizes something about this population."
      ],
      "metadata": {
        "id": "BIInEf_IgF1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is correlation?**\n",
        "\n",
        "*  Correlation measures the strength and direction of the linear relationship between two quantitative variables.\n",
        "\n",
        "* A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n",
        "**Key Characteristics:**\n",
        "*\tCorrelation coefficient (r) is less than 0 and can range from –1 to 0.\n",
        "*\tR = − 1: Perfect negative linear relationship\n",
        "*\tR = 0: No linear relationship\n",
        "*\tThe stronger the negative value (closer to –1), the stronger the inverse relationship.\n",
        "\n",
        "**Examples:**\n",
        "**1. Temperature and heating bills**\n",
        "\n",
        "* As temperature goes up, heating bills go down → negative correlation.\n",
        "\n",
        "**2. Speed and travel time (for fixed distance)**\n",
        "\n",
        "* As speed increases, time to reach destination decreases → negative correlation.\n",
        "\n",
        "**3. Absences and test scores**\n",
        "\n",
        "* More absences from class often lead to lower test scores."
      ],
      "metadata": {
        "id": "armBtlhfh0bS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "*  Machine Learning (ML) is a field of artificial intelligence (AI) that focuses on building algorithms and statistical models that enable computers to learn from data and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "**Main Components of Machine Learning**\n",
        "\n",
        "**1. Data**\n",
        "\n",
        "* The raw input used to train and test models.\n",
        "\n",
        "* Can be structured (tables) or unstructured (images, text, audio).\n",
        "\n",
        "* Quality and quantity of data heavily affect model performance.\n",
        "\n",
        "**2. Model**\n",
        "* The mathematical representation or function that maps inputs to outputs.\n",
        "\n",
        "* Examples: decision trees, neural networks, support vector machines.\n",
        "\n",
        "* The model learns patterns in the data during training.\n",
        "\n",
        "**3. Algorithm**\n",
        "* The procedure or method used to train the model.\n",
        "\n",
        "* It adjusts the model's parameters to minimize error.\n",
        "\n",
        "* Examples: gradient descent, k-means, backpropagation.\n",
        "\n",
        "**4. Training**\n",
        "* The process of feeding data into the algorithm to let the model learn.\n",
        "\n",
        "* Involves updating model parameters to reduce prediction errors.\n",
        "\n",
        "**5. Evaluation (Testing)**\n",
        "* Assessing how well the trained model performs on unseen data.\n",
        "\n",
        "* Metrics used include accuracy, precision, recall, F1 score, RMSE, etc.\n",
        "\n",
        "**6. Features**\n",
        "* Individual measurable properties or characteristics used as input.\n",
        "\n",
        "* Good feature selection and engineering improve model effectiveness.\n",
        "\n",
        "**7. Labels (for supervised learning)**\n",
        "* The correct outputs or answers provided during training.\n",
        "\n",
        "* Used to guide the learning process in supervised learning.\n",
        "\n",
        "**8. Loss Function / Objective Function**\n",
        "* Measures the difference between predicted output and actual output.\n",
        "\n",
        "* Guides the model on how to adjust its parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "jM1GMrnTiIWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        "**How the Loss Value Helps Evaluate a Model**\n",
        "\n",
        "* The loss value (or loss function output) is a key indicator of how well a machine learning model is performing during training or evaluation. It measures the difference between the model's predicted output and the true (actual) output.\n",
        "*   List item\n",
        "\n",
        "**What is a Loss Function?**\n",
        "* A loss function quantifies the error for a single prediction (or a batch of predictions). The goal of training is to minimize the loss value, meaning the model's predictions are getting closer to the correct answers.\n",
        "\n",
        "**Common examples:**\n",
        "\n",
        "* Mean Squared Error (MSE) for regression\n",
        "\n",
        "* Cross-Entropy Loss for classification\n",
        "\n",
        "**Why the Loss Value Matters**\n",
        "\n",
        "**1. Indicates model accuracy (indirectly):**\n",
        "* A low loss means predictions are close to the target values → model is learning well.\n",
        "\n",
        "* A high loss suggests large errors → model is underperforming.\n",
        "\n",
        "**2. Guides learning during training:**\n",
        "* Optimizers (like Gradient Descent) use the loss gradient to update model weights.\n",
        "\n",
        "* If loss decreases steadily → the model is improving.\n",
        "\n",
        "**3. Helps compare models:**\n",
        "* You can use loss to compare multiple models or configurations.\n",
        "\n",
        "* Lower loss on validation/test data typically indicates a better model."
      ],
      "metadata": {
        "id": "yboOjK55_7mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What are continuous and categorical variables?**\n",
        "\n",
        "**1. Continuous Variables**\n",
        "* A continuous variable can take any value within a given range, including fractions and decimals. These variables are typically measured rather than counted.\n",
        "\n",
        "**Key Features:**\n",
        "* Infinite or very large number of possible values\n",
        "\n",
        "* Can be divided meaningfully (e.g., 3.5 kg, 172.2 cm)\n",
        "\n",
        "**Examples:**\n",
        "* Height (e.g., 170.5 cm)\n",
        "\n",
        "* Temperature (e.g., 36.7°C)\n",
        "\n",
        "* Weight (e.g., 68.2 kg)\n",
        "\n",
        "* Time (e.g., 2.45 seconds)\n",
        "\n",
        "* Age (when measured precisely, e.g., 23.75 years)\n",
        "\n",
        "**2. Categorical Variables**\n",
        "* A categorical variable (also called a qualitative variable) takes on a limited, fixed number of categories or groups. These values represent labels rather than quantities.\n",
        "\n",
        "**Key Features:**\n",
        "* Data falls into distinct groups\n",
        "\n",
        "* Categories may be nominal (no order) or ordinal (with a logical order)\n",
        "\n",
        "**Examples:**\n",
        "* **Nominal (no natural order):**\n",
        "\n",
        "* Gender: Male, Female\n",
        "\n",
        "* Color: Red, Blue, Green\n",
        "\n",
        "* Marital status: Single, Married, Divorced\n",
        "\n",
        "* **Ordinal (with order):**\n",
        "\n",
        "* Education level: High School, Bachelor’s, Master’s, PhD\n",
        "\n",
        "* Satisfaction rating: Low, Medium, High\n",
        "\n"
      ],
      "metadata": {
        "id": "kiZh1FYuIzP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "\n",
        "*  Handling categorical variables is a crucial step in preparing data for machine learning models, as most models require numerical input. Here are the most common techniques for dealing with categorical variables:\n",
        "\n",
        "**1. Label Encoding**\n",
        "* **Description:** Assigns each unique category a different integer.\n",
        "\n",
        "* **use:** Useful for ordinal variables (where order matters, e.g., \"low\", \"medium\", \"high\").\n",
        "\n",
        "**Example:**\n",
        "\n",
        "    [\"red\", \"blue\", \"green\"] → [0, 1, 2]\n",
        "    \n",
        "**Downside:** For nominal variables, the model may assume a relationship between the encoded values.\n",
        "\n",
        "**2. One-Hot Encoding**\n",
        "* **Description:** Converts each category into a new binary column (1 or 0).\n",
        "\n",
        "* **use:** For nominal (unordered) categorical variables.\n",
        "\n",
        "* **Example:**\n",
        "\n",
        "      mathematica\n",
        "      [\"Red\",  \"Green\",  \"Blue\"] →\n",
        "      Red  Green   Blue\n",
        "      1      0      0\n",
        "      0      1      0\n",
        "      0      0      1\n",
        "\n",
        "* **Downside:** Can lead to high dimensionality (curse of dimensionality) for variables with many categories.\n",
        "\n",
        "**3. Binary Encoding**\n",
        "* **Description:** Converts categories to binary code and splits digits into separate columns.\n",
        "\n",
        "* **use:** When the number of categories is large.\n",
        "* **Example:**\n",
        "\n",
        "      0 → 000, 1 → 001, 2 → 010, etc.\n",
        "*  **Advantage:** Reduces dimensionality compared to one-hot encoding.\n",
        "\n",
        "**4. Target Encoding (Mean Encoding)**\n",
        "* **Description:** Replaces each category with the average of the target variable for that category.\n",
        "\n",
        "* **use:** Often used in competitions or high-cardinality features.\n",
        "\n",
        "* **Example:** If \"City\" = \"New York\" corresponds to 80% default rate, then \"New York\" → 0.8.\n",
        "\n",
        "* **Risk:** Can cause data leakage; requires cross-validation techniques to avoid it.\n",
        "\n",
        "**5. Frequency / Count Encoding**\n",
        "* **Description:** Replaces categories with their frequency or count.\n",
        "\n",
        "* **Example:**\n",
        "\n",
        "      bash\n",
        "\n",
        "      {\"A\": 100 times, \"B\": 50 times, \"C\": 10 times}\n",
        "* Becomes:\n",
        "\n",
        "      css\n",
        "\n",
        "      A → 100, B → 50, C → 10\n",
        "\n",
        "* **use**: When you suspect that frequency correlates with the target.\n",
        "\n",
        "**6. Hashing Encoding (Feature Hashing)**\n",
        "\n",
        "* **Description:** Hashes categories into a fixed number of columns using a hash function.\n",
        "\n",
        "* **use:** Large datasets with high-cardinality features (e.g., log data).\n",
        "\n",
        "* **Downside:** Potential for collisions (different values map to the same hash)."
      ],
      "metadata": {
        "id": "C7VPUJE-fbZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. What do you mean by training and testing a dataset?**\n",
        "\n",
        "*  Here's a clear breakdown:\n",
        "\n",
        "**1. Training a Dataset**\n",
        "* **Definition:** The process of feeding the model with data it uses to learn patterns, relationships, or rules.\n",
        "\n",
        "* **Goal:** To build the model by adjusting its internal parameters (like weights in a neural network or splits in a decision tree) based on the input features and the known output (target variable).\n",
        "\n",
        "* **Example:** If you're predicting house prices, the training data would include house features (size, location, number of rooms) and the actual prices.\n",
        "\n",
        "**2. Testing a Dataset**\n",
        "* **Definition:** The process of evaluating the trained model on a separate set of data that it hasn't seen before.\n",
        "\n",
        "* **Goal:** To assess how well the model generalizes to new, unseen data (i.e., performance in the real world).\n",
        "\n",
        "* **Example:** After training the house price model, you test it on a new set of houses to see how accurately it predicts their prices."
      ],
      "metadata": {
        "id": "2iTtkNMlq6nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What is sklearn.preprocessing?**\n",
        "\n",
        "*  sklearn.preprocessing is a module in scikit-learn, a popular Python machine learning library. This module provides tools and utilities to transform and prepare your data before feeding it into a machine learning model.\n",
        "\n",
        "\n",
        "**Why is preprocessing important?**\n",
        "* Real-world data can be messy: it might have different scales, missing values, or categorical variables.\n",
        "\n",
        "* Many machine learning algorithms perform better if the data is scaled, normalized, or encoded properly.\n",
        "\n",
        "* Preprocessing helps clean, transform, and format the data into a suitable form for ML models."
      ],
      "metadata": {
        "id": "pE0MNxkrsnk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What is a Test set?**\n",
        "\n",
        "* A test set is a portion of your dataset that is set aside and not used during model training. Instead, it is used after the model has been trained to evaluate how well the model performs on unseen data.\n",
        "\n",
        "\n",
        "**Key points about the Test Set:**\n",
        "* Purpose: To estimate the model’s ability to generalize — that is, to make accurate predictions on data it has never seen before.\n",
        "\n",
        "* Not used during training: The model doesn’t learn from this data; it only uses it to check performance.\n",
        "\n",
        "* Helps detect overfitting: If a model performs great on training data but poorly on the test set, it’s likely overfitting (memorizing training data rather than learning general patterns).\n",
        "\n",
        "* Usually a fixed percentage of the original dataset: Common splits are 20–30% for testing and the rest for training."
      ],
      "metadata": {
        "id": "pxlcMtuttdig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=1\n",
        ")\n",
        "\n",
        "# Fit model on training data\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7ZONDot5hCt",
        "outputId": "b9da4b49-58ae-455c-ca4b-681c1e25333e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do you approach a Machine Learning problem?**\n",
        "\n",
        "* Approaching a machine learning (ML) problem systematically ensures better results and helps avoid common pitfalls. Here's a structured step-by-step approach to solving an ML problem:\n",
        "\n",
        "**1. Understand the Problem**\n",
        "* Clarify the objective: Classification? Regression? Clustering?\n",
        "\n",
        "* Know the target variable: What are you predicting?\n",
        "\n",
        "* Business understanding: Why does this problem matter?\n",
        "\n",
        "**2. Collect and Explore the Data**\n",
        "* Get the raw data: CSV, database, API, etc.\n",
        "\n",
        "* Perform Exploratory Data Analysis (EDA):\n",
        "\n",
        " *  Summary statistics\n",
        "\n",
        "  * Data types and structure\n",
        "\n",
        "  * Class imbalance, missing values\n",
        "\n",
        "  * Visualizations (histograms, scatterplots, boxplots)\n",
        "\n",
        "**3. Preprocess the Data**\n",
        "* Handle missing values (drop, fill, impute)\n",
        "\n",
        "* Encode categorical variables (LabelEncoder, OneHotEncoder, etc.)\n",
        "\n",
        "* Scale/normalize numerical features (StandardScaler, MinMaxScaler)\n",
        "\n",
        "* Split data into training and testing sets\n",
        "\n",
        "* Feature engineering:\n",
        "\n",
        "  * Combine, transform, or create new features\n",
        "\n",
        "  * Remove irrelevant or redundant features\n",
        "\n",
        "**4. Choose the Right Model**\n",
        "* Based on problem type and data:\n",
        "\n",
        " * Classification: Logistic Regression, Random Forest, SVM, XGBoost\n",
        "\n",
        " * Regression: Linear Regression, Decision Trees, Gradient Boosting\n",
        "\n",
        " * Clustering: KMeans, DBSCAN\n",
        "\n",
        "* Start with a simple model, then try more complex ones\n",
        "\n",
        "**5. Train the Model**\n",
        "* Use the training data to fit the model:\n",
        "\n",
        "       python\n",
        "\n",
        "       model.fit(X_train, y_train)\n",
        "\n",
        "**6. Evaluate the Model**\n",
        "* Use the test set (unseen data)\n",
        "\n",
        "* Choose the right evaluation metric:\n",
        "\n",
        " * Classification: Accuracy, Precision, Recall, F1-score, ROC-AUC\n",
        "\n",
        " * Regression: MAE, RMSE, R²\n",
        "\n",
        "* Visualize performance: confusion matrix, residual plots, ROC curves\n",
        "\n",
        "**7. Tune the Model**\n",
        "* Use cross-validation (e.g., KFold)\n",
        "\n",
        "* Tune hyperparameters (e.g., with GridSearchCV, RandomizedSearchCV)\n",
        "\n",
        "* Try ensemble methods (e.g., Bagging, Boosting)\n",
        "\n",
        "**8. Test and Validate Final Model**\n",
        "* Evaluate on the final test set (if using validation split earlier)\n",
        "\n",
        "* Watch for overfitting or underfitting\n",
        "\n",
        "**9. Deploy the Model**\n",
        "* Package the model (e.g., joblib, pickle)\n",
        "\n",
        "* Create an API using Flask/FastAPI or deploy via cloud platforms\n",
        "\n",
        "* Monitor performance in production\n",
        "\n",
        "**10. Iterate and Improve**\n",
        "* Collect feedback\n",
        "\n",
        "* Update data\n",
        "\n",
        "* Re-train and re-evaluate"
      ],
      "metadata": {
        "id": "_BQ0PHOh6pul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11. Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "*  Here's why EDA matters:\n",
        "\n",
        "**1. Understand Your Data**\n",
        "* Get a sense of data types, range of values, and data distributions.\n",
        "\n",
        "* Understand what features you're working with and how they relate to the target variable.\n",
        "\n",
        "* Example: You might discover that the \"Age\" column has outliers or that the \"Gender\" column is categorical.\n",
        "\n",
        "**2. Identify and Handle Data Quality Issues**\n",
        "* Missing values: Should you fill them, drop them, or flag them?\n",
        "\n",
        "* Outliers: Do they need treatment, or are they meaningful?\n",
        "\n",
        "* Incorrect data types: Strings stored as numbers, dates stored as text, etc.\n",
        "\n",
        "* Example: A column may look numerical but actually represents categories.\n",
        "\n",
        "**3. Discover Patterns and Relationships**\n",
        "* Uncover correlations and feature importance.\n",
        "\n",
        "* Visualize relationships between features and target variables (scatter plots, boxplots, heatmaps, etc.).\n",
        "\n",
        "* Example: A strong correlation between Experience and Salary could be a valuable predictor.\n",
        "\n",
        "**4. Detect Data Imbalances**\n",
        "* In classification, check if classes are imbalanced (e.g., 90% \"No\", 10% \"Yes\").\n",
        "\n",
        "* Imbalanced data can mislead accuracy and affect model performance.\n",
        "\n",
        "* Example: You may need techniques like SMOTE or stratified sampling.\n",
        "\n",
        "**5. Inform Preprocessing Decisions**\n",
        "* EDA helps you decide:\n",
        "\n",
        "  * Which features to scale or normalize\n",
        "\n",
        " * Which features to encode\n",
        "\n",
        " * Which ones to drop\n",
        "\n",
        "* Example: Categorical features need encoding; skewed distributions may need transformation.\n",
        "\n",
        "**6. Choose the Right Model and Metric**\n",
        "* Some models assume normality, no multicollinearity, etc.\n",
        "\n",
        "* EDA guides your choice of algorithms and evaluation metrics.\n",
        "\n",
        "* Example: If the target is heavily skewed, using Mean Absolute Error (MAE) may be better than RMSE."
      ],
      "metadata": {
        "id": "G-1MlZib78P3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. What is correlation?**\n",
        "\n",
        "* Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n",
        "\n",
        "**Example**\n",
        "\n",
        "     Hours Studied\tExam Score\n",
        "     1\t              50\n",
        "     2\t              60\n",
        "     3\t              70\n",
        "     4\t              80"
      ],
      "metadata": {
        "id": "ivFSkjcl9CWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13. What does negative correlation mean?**\n",
        "\n",
        "*  A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n",
        "**In Simple Terms:**\n",
        "* When X goes up, Y goes down — and vice versa.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "    Hours Watching TV\tExam Score\n",
        "          5                50\n",
        "          4                60\n",
        "          3                70\n",
        "          2                80\n",
        "          1                90\n",
        "\n",
        "* As time spent watching TV increases, exam score decreases → ❗️Negative correlation.\n",
        "\n"
      ],
      "metadata": {
        "id": "0CjPdPbT-UHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14. How can you find correlation between variables in Python?**\n",
        "\n",
        "*  Here are some ways to find correlation:\n",
        "\n",
        "\n",
        "**1. Using Pandas .corr() method**\n",
        "* If you have a DataFrame, this computes the correlation matrix between all numerical columns:\n",
        "\n",
        "      import pandas as pd\n",
        "\n",
        "      # Example DataFrame\n",
        "      df = pd.DataFrame({\n",
        "          'age': [25, 32, 47, 51, 62],\n",
        "          'income': [50000, 60000, 80000, 90000, 100000],\n",
        "          'expenses': [20000, 25000, 30000, 35000, 40000]\n",
        "      })\n",
        "\n",
        "      # Correlation matrix\n",
        "      corr_matrix = df.corr()\n",
        "      print(corr_matrix)\n",
        "\n",
        "**2. Correlation between two specific variables**\n",
        "\n",
        "    corr = df['age'].corr(df['income'])\n",
        "    print(\"Correlation between age and income:\", corr)\n",
        "\n",
        "**3. Using NumPy's corrcoef**\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    corr_coef = np.corrcoef(df['age'], df['income'])[0, 1]\n",
        "    print(\"Correlation coefficient:\", corr_coef)\n",
        "\n",
        "**4. Visualizing correlation matrix with Seaborn heatmap**\n",
        "\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "DpIq6_FT_cLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15. What is causation? Explain difference between correlation and causation with an example.**\n",
        "\n",
        "* Causation means that one variable directly affects another — a cause-and-effect relationship.\n",
        "In other words:\n",
        "\n",
        "* If X causes Y, then changing X will change Y.\n",
        "\n",
        "\n",
        "**Difference between correlation and causation**\n",
        "\n",
        "**Correlation**\n",
        "\n",
        "* **Definition:** A mutual relationship between variables\n",
        "\n",
        "* **Direction:** \tNo direction implied\n",
        "\n",
        "* **Implied Cause:**  No\n",
        "\n",
        "* **Testing:**  Statistical measures (e.g., Pearson)\n",
        "\n",
        "* **Example:**\tIce cream ↑ and drowning ↑\n",
        "\n",
        "**Causation**\n",
        "\n",
        "* **Definition:** A direct cause-effect relationship\n",
        "\n",
        "* **Direction:** One variable directly affects the other\n",
        "\n",
        "* **Implied Cause:** Yes\n",
        "\n",
        "* **Testing:** Requires controlled experiments or domain knowledge\n",
        "\n",
        "* **Example:** Smoking → Lung cancer"
      ],
      "metadata": {
        "id": "2X01mYkBBX1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16. What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "*  An optimizer is an algorithm used to adjust the model’s parameters (like weights and biases) to minimize the loss function. In simple terms, it helps your model learn from data by making it better at predicting the correct output.\n",
        "\n",
        "\n",
        "**Types of Optimizers**\n",
        "\n",
        "**1. Gradient Descent (Batch Gradient Descent)**\n",
        "*\tUses the entire dataset to compute the gradient before updating the parameters.\n",
        "* Update Rule:\n",
        "\n",
        "    *θ = θ – α ⋅ ∇ J (θ)*\n",
        "\n",
        "*\tPros: Stable convergence\n",
        "*\tCons: Slow for large datasets\n",
        "* Example (Conceptual):\n",
        "\n",
        "      # No specific library function; this is manually implemented in basic ML models\n",
        "\n",
        "**2. Stochastic Gradient Descent (SGD)**\n",
        "* Updates weights for each training example (sample).\n",
        "\n",
        "* Introduces noise, which can help escape local minima.\n",
        "\n",
        "* Example (PyTorch):\n",
        "\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "* Pros: Faster updates, handles large datasets\n",
        "\n",
        "* Cons: More fluctuation, less stable\n",
        "\n",
        "**3. Mini-Batch Gradient Descent**\n",
        "* Uses a subset of the data (batch) for each update.\n",
        "\n",
        "* A balance between batch GD and SGD.\n",
        "\n",
        "* Example:\n",
        "* Usually controlled by setting the batch_size in your DataLoader in PyTorch or Keras.\n",
        "\n",
        "      # Mini-batch handled by DataLoader or training loop\n",
        "\n",
        "**4. Momentum**\n",
        "* Helps accelerate SGD by using a velocity vector to smooth updates.\n",
        "\n",
        "* Reduces oscillation in high-curvature areas.\n",
        "\n",
        "* Example:\n",
        "\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "**5. Adagrad (Adaptive Gradient Algorithm)**\n",
        "* Adjusts learning rate for each parameter individually based on past gradients.\n",
        "\n",
        "* Good for sparse data.\n",
        "\n",
        "* Example:\n",
        "\n",
        "      optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
        "* Cons: Learning rate decays too much over time.\n",
        "\n",
        "**6. RMSprop (Root Mean Square Propagation)**\n",
        "* Like Adagrad but with a moving average of squared gradients to prevent decay of learning rate.\n",
        "\n",
        "* Works well for non-stationary problems like RNNs.\n",
        "\n",
        "* Example:\n",
        "\n",
        "      optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "**7. Adam (Adaptive Moment Estimation)**\n",
        "* Combines ideas from Momentum and RMSprop.\n",
        "\n",
        "* Maintains running averages of both gradients and squared gradients.\n",
        "\n",
        "* Example:\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "* Pros: Fast, reliable, and widely used\n",
        "\n",
        "* Best default choice for most deep learning tasks\n",
        "\n",
        "**8. AdamW (Weight Decay Regularization)**\n",
        "* A variant of Adam that decouples weight decay from the gradient update.\n",
        "\n",
        "* Better generalization for deep models (especially transformers).\n",
        "\n",
        "* Example:\n",
        "\n",
        "      optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n"
      ],
      "metadata": {
        "id": "aZXeMhU0eI3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17. What is sklearn.linear_model?**\n",
        "\n",
        "* sklearn.linear_model is a module in Scikit-learn that contains classes and functions for building linear models — models that make predictions based on linear relationships between features and the target variable.\n",
        "\n",
        "**What Does It Do?**\n",
        "* It provides tools to solve:\n",
        "\n",
        "* Regression problems (predicting continuous values)\n",
        "\n",
        "* Classification problems (predicting categories)\n",
        "\n"
      ],
      "metadata": {
        "id": "pFtEljjM6FxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18. What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "*  The .fit() method is used to train a machine learning model on your data. It finds the best parameters (like weights or coefficients) that minimize the loss function and allow the model to make accurate predictions.\n",
        "\n",
        "**What Happens When You Call .fit()?**\n",
        "* Takes your input data (features) and target values (labels)\n",
        "\n",
        "* Computes how well the model is doing (using a loss function)\n",
        "\n",
        "* Optimizes model parameters to minimize the error\n",
        "\n",
        "* Stores the trained model parameters internally\n",
        "\n",
        "**Required Arguments:**\n",
        "\n",
        "\n",
        "| Argument | Description                          | Required |\n",
        "|----------|--------------------------------------|----------|\n",
        "| `X`      | Feature matrix (inputs), shape: `(n_samples, n_features)` | ✅ Yes |\n",
        "| `y`      | Target labels/values (outputs), shape: `(n_samples,)`     | ✅ Yes |\n",
        "\n",
        "---\n",
        "\n",
        "##  Example: Using `LinearRegression`\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "# X = features, y = target\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "T5hcPKZq69yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19. What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "* The model.predict() method is used after training a model (using fit()), and it is used to make predictions on new or unseen input data.\n",
        "\n",
        "* In simple terms: predict() uses the model's learned patterns to output predictions based on input features.\n",
        "\n",
        "**Arguments for model.predict()**\n",
        "\n",
        "\n",
        "| Argument | Description                          | Required |\n",
        "|----------|--------------------------------------|----------|\n",
        "| `X`      | Input features for prediction, shape: `(n_samples, n_features)` | ✅ Yes |\n",
        "\n",
        "---\n",
        "\n",
        "##  Example:\n",
        "\n",
        "```python\n",
        "# Assuming model is already trained via model.fit()\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "RtM2akjv84fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20. What are continuous and categorical variables?**\n",
        "\n",
        "**Continuous Variables**\n",
        "* **Definition:** Variables that can take any numeric value within a range.\n",
        "\n",
        "* Usually measured quantities.\n",
        "\n",
        "* Values are often decimals/floats, not just integers.\n",
        "\n",
        "**Examples:**\n",
        " * Height (e.g., 170.5 cm)\n",
        "\n",
        " * Temperature (e.g., 23.8°C)\n",
        "\n",
        " * Weight (e.g., 65.2 kg)\n",
        "\n",
        " * Time taken (e.g., 12.45 seconds)\n",
        "\n",
        "\n",
        "**Categorical Variables**\n",
        "* **Definition:** Variables that represent categories or groups.\n",
        "\n",
        "* Usually qualitative (non-numeric) or numeric codes representing groups.\n",
        "\n",
        "* Limited number of possible values (called levels or classes).\n",
        "\n",
        "**Types of Categorical Variables:**\n",
        "* Nominal: No natural order (e.g., color: red, blue, green)\n",
        "\n",
        "* Ordinal: Have a meaningful order (e.g., rating: low, medium, high)\n",
        "\n",
        "**Examples:**\n",
        "* Gender (male, female)\n",
        "\n",
        "* Color (red, blue, green)\n",
        "\n",
        "* Education level (high school, bachelor, master)\n",
        "\n",
        "* Product category (electronics, clothing, groceries)\n",
        "\n"
      ],
      "metadata": {
        "id": "B6mvwadj_NXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21. What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "* Feature scaling is the process of normalizing or standardizing numerical features so they share a common scale, without distorting differences in the ranges of values.\n",
        "\n",
        "**How does feature scaling help in Machine Learning?**\n",
        "\n",
        "**1. Faster convergence in optimization algorithms**\n",
        "* Algorithms like Gradient Descent perform better when features are on a similar scale, speeding up training.\n",
        "\n",
        "**2. Improved accuracy**\n",
        "* Models that rely on distance or magnitude (like KNN, SVM, or K-means) perform better with scaled data.\n",
        "\n",
        "**3. Prevents bias toward features with large values**\n",
        "* Without scaling, features with bigger ranges might dominate the learning process.\n",
        "\n",
        "**4. Makes coefficients more interpretable**\n",
        "* In linear models, scaling helps to compare feature importances directly."
      ],
      "metadata": {
        "id": "_T1BAsmeAHp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22. How do we perform scaling in Python?**\n",
        "\n",
        "**Step-by-Step: How to Perform Feature Scaling in Python**\n",
        "\n",
        "**1. Import the necessary scaler**\n",
        "\n",
        "Scikit-learn provides different scalers for different types of scaling:\n",
        "\n",
        "* StandardScaler – for standardization (mean = 0, std = 1)\n",
        "\n",
        "* MinMaxScaler – for normalization (scale to [0, 1])\n",
        "\n",
        "* RobustScaler – for scaling with outliers\n",
        "\n",
        "* MaxAbsScaler – scales based on max absolute value\n",
        "\n",
        "**2. Example with StandardScaler (most common)**\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import pandas as pd\n",
        "\n",
        "    # Example data\n",
        "    data = {'height': [150, 160, 170, 180],\n",
        "            'weight': [50, 60, 65, 80]}\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Create the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform the data\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    # Convert back to DataFrame\n",
        "    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "    print(scaled_df)\n",
        "\n",
        "**3. Using MinMaxScaler (scale to [0, 1])**\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "    print(scaled_df)"
      ],
      "metadata": {
        "id": "5BdpoLAT_Za6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23. What is sklearn.preprocessing?**\n",
        "\n",
        "* sklearn.preprocessing is a module in the Scikit-learn library that provides a variety of data preprocessing tools used to prepare your dataset before feeding it into a machine learning model.\n"
      ],
      "metadata": {
        "id": "f0t8QH5jCSXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24. How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "*  In Python, the most common way to split data for model fitting — into training and testing sets — is by using the train_test_split() function from scikit-learn. This function randomly splits arrays or matrices into training and testing subsets.\n",
        "\n",
        "\n",
        "* Here's a step-by-step example:\n",
        "\n",
        "**1. Import the necessary modules**\n",
        "\n",
        "       from sklearn.model_selection import train_test_split\n",
        "\n",
        "**2. Assume you have your features X and target y**\n",
        "* For example:\n",
        "\n",
        "       X = df.drop('target_column', axis=1)  # Features\n",
        "       y = df['target_column']              # Target\n",
        "\n",
        "**3. Split the data**\n",
        "\n",
        "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "**Parameters:**\n",
        "*  test_size=0.2: 20% of the data will be used for testing.\n",
        "\n",
        "* random_state=42: Ensures reproducibility by fixing the random seed.\n",
        "\n",
        "**4. Now you can train your model on X_train, y_train and test it on X_test, y_test.**\n",
        "\n",
        "**Optional Parameters:**\n",
        "* shuffle=True: Whether or not to shuffle the data before splitting (default is True).\n",
        "\n",
        "* stratify=y: Ensures that the proportion of classes is maintained (useful for classification tasks).\n",
        "\n",
        "**Example with stratification:**\n",
        "\n",
        "\n",
        "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n"
      ],
      "metadata": {
        "id": "tXsb5pvqD9Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25. Explain data encoding?**\n",
        "\n",
        "**What is Data Encoding?**\n",
        "* Data encoding is the process of converting categorical data (non-numeric, like labels or text) into a numeric format so that it can be used in machine learning models, which typically require numerical input.\n",
        "\n",
        "\n",
        "**Why is Encoding Important?**\n",
        "* Machine learning algorithms (like linear regression, decision trees, or neural networks) cannot process strings or categorical variables directly. They need numerical representations.\n",
        "\n",
        "* For example, you can't feed ['red', 'green', 'blue'] into a model. These must be encoded numerically first.\n",
        "\n",
        "**Common Data Encoding Techniques**\n",
        "\n",
        "**1. Label Encoding**\n",
        "* Converts each category into a unique integer.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    colors = ['red', 'green', 'blue', 'green']\n",
        "    encoded = le.fit_transform(colors)\n",
        "    print(encoded)  # [2 1 0 1]\n",
        "\n",
        "**2. One-Hot Encoding**\n",
        "* Creates a binary column for each category and marks the presence with 1 and absence with 0.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue', 'Green']})\n",
        "    encoded_df = pd.get_dummies(df, columns=['Color'])\n",
        "    print(encoded_df)\n",
        "\n",
        "**Output:**\n",
        "\n",
        "       Color_Blue  Color_Green  Color_Red\n",
        "    0           0            0          1\n",
        "    1           0            1          0\n",
        "    2           1            0          0\n",
        "    3           0            1          0\n",
        "\n",
        "**3. Ordinal Encoding**\n",
        "* Maps each category to an integer based on its order.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "    from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "    oe = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "    data = [['Low'], ['Medium'], ['High']]\n",
        "    encoded = oe.fit_transform(data)\n",
        "\n",
        "\n",
        "**4. Binary Encoding / Target Encoding / Hashing Encoding**\n",
        "* Used for high-cardinality features (e.g., country names, product codes). * These are more advanced and useful when:\n",
        "\n",
        "* Too many unique values for one-hot encoding\n",
        "\n",
        "* Want to reduce dimensionality\n",
        "\n",
        "**Example libraries:** category_encoders (not in sklearn)"
      ],
      "metadata": {
        "id": "wy2wolu1KP9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_XOB4qWXNObZ"
      }
    }
  ]
}